---
title: 4. Linear Transformations
description: All about linear transformations.
authors: [Bhanu]
date: 2023-07-09
---

## What is a Linear Transformation
    We are now moving into some more applications of the basic concepts we spent lectures 1-3 learning. Good job on making it this far, stuff gets a little more interesting (and difficult) here.

    This whole lesson we will be learning linear transformations, which are one of the most important things in all of mathematics. Every single field uses linear transformations, and you've used linear transformations yourself without even realizing it.

## Linear Combinations & How They Relate
    Before we jump straight to linear transformations, lets take a baby step to linear combinations first. A linear combination is exactly what it sounds like: a way to combine vectors together linearly (addition/subtraction or multiplication).

$$
\begin{align*}
3

\begin{bmatrix}
3 \\
2 \\
1 \\
\end{bmatrix}
- \frac{1}{2}
\begin{bmatrix}
-2 \\
4 \\
0 \\
\end{bmatrix}
=
\begin{bmatrix}
10 \\
4 \\
3 \\
\end{bmatrix}
\end{align*}
$$

This is an example of a __linear combination__, because 2 vectors are being combined by using scalar multiplication and addition. Now lets make this a little bit more complicated to start moving towards linear transformations.

Lets consider a linear combination where the scalars are variables.

$$
\begin{align*}
x_1

\begin{bmatrix}
-1 \\
2 \\
\end{bmatrix}
+ x_2
\begin{bmatrix}
3 \\
1 \\
\end{bmatrix}
    = 
\begin{bmatrix}
y_1 \\
y_2 \\
\end{bmatrix}
\end{align*}
$$

We can clearly see that this is a linear combination where we take $$ x_1 $$ multiples of our first vector, and add $$ x_2 $$ multiples of our second vector to get some final vector with $$y_1$$ and $$y_2$$. But let's multiply this out and see what it looks like as a __system of equations__, like we saw in lecture 3.

$$ 
\begin{align}
y_1 = -x_1 + 3x_2 \\
y_2 = 2x_1 + x_2 \\
\end{align}
$$

So we see now that systems of equations are analogous to linear combinations, and finding a solution for a system of equations is the same as finding coefficients to use in a linear combination to get some vector.

Finally, lets connect this all to matrices, and then to linear transformations, which we will soon see are very closely related to matrices.

Looking at the system of equations, we can also represent it now as the coefficient matrix, like this:

$$ 
\begin{align*}

\begin{bmatrix}
y_1\\
y_2\\
\end{bmatrix}
    = 
\begin{bmatrix}
-1 & 3 \\
2 & 1 \\
\end{bmatrix}
\begin{bmatrix}
x_1\\
x_2\\
\end{bmatrix}

\end{align*}
$$

This is __very important!!!__ We now see that linear combinations are nothing more than a way to represent systems of equations, which in turn are a way to visualize matrix multiplication. But what is the underlying reasoning for all of this? __Linear Transformations!__

We see from the matrix representation that all we are actually doing is taking some vector in $$\mathbb{R^2}$$, made up of $$ x_1 $$ and $$x_2$$, and multiplying it by a matrix in order to get a new vector in $$\mathbb{R^2}$$, made up of $$y_1$$ and $$y_2$$. It's now clear to see that the matrix we use _transforms_ the x vector into our new y vector. __We are using the matrix to apply a linear transformation to the vector.__ 

This is the basic idea of what a linear transformation is, and we will get into its properties and impact next, but for now it is important to understand that vectors, matrices, systems of equations, and linear combinations all combine to give us this one concept of a linear transformation, and its __one of the most important parts of linear algebra__, so take your time to understand it.

## Linear Transformation

Now lets get into how to actually talk about linear transformations. A linear transformation is a set of operations that takes us from a domain in $$\mathbb{R^n}$$ to a domain in $$\mathbb{R^m}$$. This means that it takes us from one dimension to another.
If you remember from our section about matrices, this is the same definition we got for an application of matrices. We've finally gotten to that application, and so we now know that linear transformations can be represented as matrices. This is shown by saying "Linear transformation _T_ given by the matrix A" where A will be some n x m matrix.

Remember that a transformation takes us from one dimension to another, so a transformation represented by an n x m matrix takes us from $$\mathbb{R^m}$$ to $$\mathbb{R^n}$$. This makes sense because if we have a vector in $$\mathbb{R^2}$$ and we multiply it by a 3 x 2 matrix, we end up with a vector in $$\mathbb{R^3}$$:
$$
\begin{align*}
\begin{bmatrix}
1 & 1 \\
0 & 2 \\
-1 & 1 \\
\end{bmatrix}
\begin{bmatrix}
-2 \\
1 \\
\end{bmatrix}
=
\begin{bmatrix}
-1 \\
2 \\
3 \\
\end{bmatrix}
\end{align*}
$$

In this example, the linear transformation we are applying would be represented by the 3x2 matrix. This is important to keep in mind: although linear transformations are a new concept and a very fundamental one, at their core they're just about multiplication by a matrix.

## Linearity

So what does it actually mean for something to be a linear transformation? Well this is where linearity comes in. By definition, all linear transformations must have this property known as linearity, which boils down to 2 simple checks.

For something to be linear, it must have __additivity__ and __homogeniety__. These are big words, but they aren't hard to understand. Here is an example:

Lets take the simplest linear function possible: 

$$
f(x) = x
$$

For this to have additivity all it means is that if you take the function of 2 inputs, it has to equal the sum of the function of each input.
$$
f(a+b) = f(a) + f(b)
$$

Let's check with a = 2 and b = 3:

$$
\begin{align*}
f(2+3) &= f(5) = 5 \\
f(2) + f(3) &= 2 + 3 = 5 \\
f(2+3) &= f(2) + f(3)\\
\end{align*}
$$

Now lets check homogeniety. For this, the function of a scalar multiple of an input must equal the scalar multiple of the function value of that input.

$$
k \cdot f(a) = f(k \cdot a)
$$

Lets check with a = 2 and k = 5:

$$
\begin{align*}
5 \cdot f(2) = 5 \cdot 2 = 10 \\
f(5 \cdot 2) = f(10) = 10 \\
5 \cdot f(2) = f(5 \cdot 2) \\
\end{align*}
$$

So if an operation holds to these 2 conditions, we say that the operation has linearity and so is a linear transformation. This is the precise definition of a linear transformation.

__IMPORTANT:__ Here is a little bit of a cheat, matrices always have linearity. This is why all linear transformations can be shown as a matrix and vice versa. (All matrices describe some linear transformation.)
## Inverse
Here we will briefly discuss the concept of an inverse and what it means. 

Consider a linear transformation _T_ given by matrix A. If we transform some vector _v_, we get a new vector, lets call it _u_. This can be shown as:

$$
Av = u
$$

But now lets say we're given the vector _u_ and we want to know what vector _v_ it corresponds to? This is where inverses come in. This lets us apply the transformation backwards, and go from output to input. So we want to find the function such that:

$$
v = f(u)
$$

Lets say matrix B represents the inverse of the linear transformation _T_, denoted by $$ T^{-1} $$. This means that if you apply B to the output, you get the input. Then:

$$
v = Bu
$$

So we can also define that matrix B is the inverse of matrix A, because it describes the inverse of the linear transformation given by A. Thus:

$$
B = A^{-1}
$$

What does this all mean? It means that (sometimes) if we have a linear transformation taking us from an input to an output, we can find the inverse linear transformation that takes us from the output to the input. This can be done as long as the matrix of the linear transformation is __invertible.__ We will discuss invertibility further later.

## Injective

Lets return back to linear transformations to begin working our way towards a definition of invertibility. First, we define what it means for a transformation to be injective.

For a transformation to be injective (also knows as "one-to-one" functions), it simply means that each output is mapped to only 1 corresponding input. Visually this can be seen like this: __NEED PICTURE__

An example of an injective function is [$$y = x^3$$](https://www.desmos.com/calculator/qtupcw26mk). This has each output mapping to only one input, and a way to see this is with the "horizontal line test". If you were to pass a horizontal line from the top to the bottom of the function, nowhere does it hit 2 points simultaneously (try it on the link).

Injective functions are not necessarily invertible because not all outputs have a corresponding input. Thus the inverse would not be defined at some points. This can be seen with the function [$$ e^{x} $$](https://www.desmos.com/calculator/bzlf5smk5h), which doesn't have any inputs corresponding to the negative numbers.

(Note you can kind of make injective functions invertible by just redefining the output space to ignore the area the function doesn't map to, but I don't believe this is too important to understand for now.)
## Surjective

Surjective functions (also known as "onto" functions) are the second type of function we will discuss. For a surjective function, all inputs are mapped to an output, with no outputs left unmatched to. However, an output can have multiple inputs matching to it. Visually this can be seen as this: __NEED IMAGE__.

An example of a surjective function is [$$ y = x^{3} - x $$](https://www.desmos.com/calculator/5yvibjqwmc), which does not pass the horizontal line test, but does match to all possible real numbers as outputs.

Surjective functions are not necessarily invertible because an output can have multiple inputs mapping to it, which would make the inverse impossible to determine at those points, because you would get 2 answers.

## Bijective

Finally, a bijective function is simple one that is both injective and surjective. This means that all the outputs must have a corresponding input, and multiple inputs cannot map to the same output. This can be seen visually like this: __NEED IMAGE__.

An example of a bijective function is [$$y = 2x$$](https://www.desmos.com/calculator/lqvf6j2fje). This function has all the outputs mapped to an input, because the range is all real numbers, so it is surjective. In addition, it passes the horizontal line test because no output has 2 (or more) corresponding inputs, so it is injective.

This means the function is bijective. Now, by definition, which we will not prove here, bijective functions are invertible. But this should make logical sense, because all it means to be bijective is that all the outputs: 1. Can be mapped to an input and 2. Can't be mapped to multiple inputs. This just means that we can be sure that if a function is bijective and we are given an output, we can determine the input it came from, which is exactly what an inverse is.

## Invertibility

Finally let's combine all of this together. And don't worry we will explain why invertibility matters in the next section, just stick with us for now.

So if a function is bijective, that means that every input it has maps to exactly one output. That means if you knew the output, you could trace it back to the input, as seen here:

**_/insert example of input-output tracing/_**

How do you trace it back? Well you do the opposite of what you did to go from input to output, and this opposite is what we call the __inverse__. 

Not all functions are invertible, as we discussed throughout this chapter, but thankfully for us it is very easy to see if a linear transformation is invertible given its matrix. All we have to do is take it to its RREF form. If the RREF form of a matrix is the identity matrix, then the matrix (and the linear transformation associated with the matrix) is invertible. We won't do the proof here, but you can read it elsewhere if you're curious.

Here is an example of an invertible matrix:

$$
A=\begin{bmatrix}
     2 & 0 & -1 \\
     5 & 1 & 0 \\
     0 & 1 & 3
\end{bmatrix}
$$

In order to find the inverse of this matrix, we can place this matrix and augment it with the identify matrix of the same size, then place the matirx into RREF. The augmented matrix will then be the inverted matirx. 
$$
\left[\begin{array}{ccc|ccc}  
    2 & 0 & -1 & 1 & 0 & 0  \\
    5 & 1 & 0 & 0 & 1 & 0 \\
    0 & 1 & 3 & 0 & 0 & 1
  \end{array}\right]
$$

Below you will find the steps to find the inverted matrix:

$$
\begin{array}{c|c} 
  \left[\begin{array}{ccc|ccc}  
    1 & 0 & -1/2 & 1/2 & 0 & 0  \\
    5 & 1 & 0 & 0 & 1 & 0 \\
    0 & 1 & 3 & 0 & 0 & 1
  \end{array}\right] & 
  \begin{align*} 
    R_1 & = R_1/2
  \end{align*}  \\ \\ \hline \\

  \left[\begin{array}{ccc|ccc}  
    1 & 0 & -1/2 & 1/2 & 0 & 0  \\
    0 & 1 & 5/2 & -5/2 & 1 & 0 \\
    0 & 1 & 3 & 0 & 0 & 1
  \end{array}\right] & 
  \begin{align*} 
    R_2 & = R_2 - 5R_1
  \end{align*}  \\ \\ \hline \\

  \left[\begin{array}{ccc|ccc}  
    1 & 0 & -1/2 & 1/2 & 0 & 0  \\
    0 & 1 & 5/2 & -5/2 & 1 & 0 \\
    0 & 0 & 1/2 & 5/2 & -1 & 1
  \end{array}\right] & 
  \begin{align*} 
    R_3 & = R_3 - R_2
  \end{align*}  \\ \\ \hline \\

  \left[\begin{array}{ccc|ccc}  
    1 & 0 & -1/2 & 1/2 & 0 & 0  \\
    0 & 1 & 5/2 & -5/2 & 1 & 0 \\
    0 & 0 & 1 & 5 & -2 & 2
  \end{array}\right] & 
  \begin{align*} 
    R_3 & = 2R_3
  \end{align*}  \\ \\ \hline \\
  
  \left[\begin{array}{ccc|ccc}  
    1 & 0 & 0 & 3 & -1 & 1 \\
    0 & 1 & 0 & 10 & 6 & -5 \\
    0 & 0 & 1 & 5 & -2 & 2
  \end{array}\right] & 
  \begin{align*} 
    R_1 & = R_1 + \frac{1}{2}R_3 \\
    R_2 & = R_2 -\frac{5}{2}R_3 \\
  \end{align*}  \\ \\ \hline \\
\end{array}
$$

Now that the matrix is in RREF form, we can determine that the inverse matrix is:

$$
A^{-1} = \begin{bmatrix}
    3 & -1 & 1 \\
    10 & 6 & -5 \\
    5 & -2 & 2
\end{bmatrix}
$$

### Conditions of Invertability
1. Matrix must be square
   - This should make sense since when solving for the inverse, we set it equal to the identify matrix, if the matrix is not square, it cannot have an identity matrix with 1's along the diagonal for all rows
2. A matrix is invertable if and only if its determinant is not equal to 0 ($$\text{det}(A) \neq 0$$)
    - Determinants can only be solved for in square matricies which furthers the first point
  
  We touched on determinants briefly when explaining how to take the cross product of two vectors but we'll explain more of what this means in a future lesson.

## Why You Should Care

Finally we can talk about why all of this matters. Well, we use linear transformations for all sorts of things, but let's discuss it in the context of data transfer. When you send data (a vector) from one computer to another, you might want to encode it so people can't read it, like if you're transferring a credit card number. Well to do this, you can simply transform the vector by multiplying it by a matrix. However, you want this transformation to be invertible, so that the person receiving the new data can unscramble it to find the original message. This is where you'd need an invertible linear transformation.

Additionally, all of this information is mostly a foundation for us to get into the later parts of linear algebra, which will be all about using matrices and their inverses to compute calculations that are so complex, calculators can't even do them. 

To put it briefly, inverses are vital because this gives us a vital tool we need in our toolkit to dive into the second half of linear algebra (which we still won't do yet because there are some loose ends to tie up.)


## Practice

